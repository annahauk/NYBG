{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications.xception import preprocess_input as preprocess_xception\n",
    "from tensorflow.keras.applications import Xception\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Input\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n",
    "from torchvision.models.detection import MaskRCNN_ResNet50_FPN_Weights, maskrcnn_resnet50_fpn\n",
    "from torchvision.transforms import Compose, ToTensor, Normalize\n",
    "import torch\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_gaussian_blur_and_edge_detection(image_np):\n",
    "    # Apply modifications directly on NumPy array for efficiency\n",
    "    # print(image_np.shape, image_np.dtype, type(image_np))\n",
    "    image_np = cv2.convertScaleAbs(image_np, alpha=1.1, beta=-100)\n",
    "    blurred = cv2.GaussianBlur(image_np, (5, 5), sigmaX=10)\n",
    "    gray = cv2.cvtColor(blurred, cv2.COLOR_RGB2GRAY)\n",
    "    edges = cv2.Canny(gray, threshold1=10, threshold2=95, L2gradient=True)\n",
    "    closing = cv2.morphologyEx(edges, cv2.MORPH_CLOSE, np.ones((3, 3), np.uint8))\n",
    "    edges_3channel = np.stack((closing,) * 3, axis=-1)\n",
    "    dilated_edges = cv2.dilate(edges_3channel, None, iterations=1)\n",
    "    return cv2.addWeighted(image_np, 1, dilated_edges, 0.5, 0)\n",
    "\n",
    "class ImageAugmentation(object):\n",
    "    def __call__(self, x):\n",
    "        return apply_gaussian_blur_and_edge_detection(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_instance_segmentation(num_classes):\n",
    "    \n",
    "    model = maskrcnn_resnet50_fpn(weights=MaskRCNN_ResNet50_FPN_Weights.DEFAULT)\n",
    "    # Get number of input features for the classifier\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    # Replace the pre-trained head with a new one\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "    \n",
    "    # Now get the number of input features for the mask predictor and replace the mask head with a new one\n",
    "    in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels\n",
    "    hidden_layer = 256\n",
    "    model.roi_heads.mask_predictor = MaskRCNNPredictor(in_features_mask, hidden_layer, num_classes)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_model = get_model_instance_segmentation(5+1) # 5 classes + background\n",
    "mask_model = mask_model.to(device)\n",
    "# Loading a pre-existing trained model parameters\n",
    "model_state_dict = torch.load('model_weights.pth', map_location=device)\n",
    "mask_model.load_state_dict(model_state_dict)\n",
    "mask_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transform():\n",
    "    return Compose([\n",
    "        ImageAugmentation(),\n",
    "        ToTensor(),\n",
    "        Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_mask(image_np, model, threshold = 0.9):\n",
    "    transformations = get_transform()\n",
    "    image_tensor = transformations(image_np).unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        prediction = model(image_tensor)\n",
    "    if prediction[0]['masks'].size(0) > 0:\n",
    "        mask = (prediction[0]['masks'][0, 0] > threshold).cpu().numpy()\n",
    "        image_np[mask] = 255\n",
    "        return image_np\n",
    "    else:\n",
    "        return image_np\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename_train = \"./bttai-nybg-2024/BTTAIxNYBG-train.csv\"\n",
    "df_train = pd.read_csv(filename_train)\n",
    "filename_test = \"./bttai-nybg-2024/BTTAIxNYBG-test.csv\"\n",
    "df_test = pd.read_csv(filename_test)\n",
    "filename_val = \"./bttai-nybg-2024/BTTAIxNYBG-validation.csv\"\n",
    "df_val = pd.read_csv(filename_val)\n",
    "train_image_directory = \"./bttai-nybg-2024/BTTAIxNYBG-train/BTTAIxNYBG-train\"\n",
    "validation_image_directory = \"./bttai-nybg-2024/BTTAIxNYBG-validation/BTTAIxNYBG-validation\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_and_mask(img):\n",
    "    image = apply_mask((255*img).astype(np.uint8), mask_model)\n",
    "    image = preprocess_xception(image)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tensor = Input(shape = (1000, 1000, 3))\n",
    "xception_base = Xception(include_top=False, weights='imagenet', input_tensor = input_tensor)\n",
    "classes = list(df_train[\"classLabel\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xception_output = GlobalAveragePooling2D()(xception_base.output)\n",
    "xception_output = Dense(1024, activation = 'relu')(xception_output)\n",
    "predictions = Dense(len(classes), activation = 'sigmoid')(xception_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(inputs = xception_base.input, outputs = predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in xception_base.layers:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer = 'adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(preprocessing_function = preprocess_and_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_generator = datagen.flow_from_dataframe(\n",
    "    df_val,\n",
    "    directory=validation_image_directory,\n",
    "    x_col='imageFile',\n",
    "    y_col='classLabel',\n",
    "    target_size=(1000, 1000),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical',\n",
    "    shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# def show_image(image, index):\n",
    "#     # If preprocessing includes normalization, adjust the image to display correctly\n",
    "#     image = image[index]\n",
    "#     if np.min(image) < 0:\n",
    "#         # Rescale to 0-1 if preprocessing involves standardization\n",
    "#         image = (image - image.min()) / (image.max() - image.min())\n",
    "\n",
    "#     plt.imshow(image)\n",
    "#     plt.title(\"Sample Image\")\n",
    "#     plt.axis('off')\n",
    "#     plt.show()\n",
    "\n",
    "# show_image(x, 14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = datagen.flow_from_dataframe(\n",
    "    df_train,\n",
    "    directory=train_image_directory,\n",
    "    x_col='imageFile',\n",
    "    y_col='classLabel',\n",
    "    target_size=(1000, 1000),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical',\n",
    "    shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',  # Monitor validation loss\n",
    "    patience=3,  # number of epochs with no improvement after which training will be stopped\n",
    "    verbose=1,\n",
    "    mode='min',  # the training will stop when the quantity monitored has stopped decreasing\n",
    "    restore_best_weights=True  # restore model weights from the epoch with the best value of the monitored quantity\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=len(train_generator),\n",
    "    epochs=10,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=50,\n",
    "    callbacks=[early_stopping])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
